{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECG visualization",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrizfJg3sLmM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-81n65Us8sL"
      },
      "source": [
        "pip install patool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPRzHLnhSCzz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2JBYMBQuuyk"
      },
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive('/content/train (2).rar',outdir='/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A8XkF_LswFh"
      },
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive('/content/test (1).rar',outdir='/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBOUZrU3XbAa"
      },
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive('/content/gdrive/My Drive/trainecg.rar',outdir='/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxV_PVjHtEhp"
      },
      "source": [
        "import glob\n",
        "import scipy.misc\n",
        "from numpy import genfromtxt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "from io import BytesIO\n",
        "\n",
        "if not os.path.exists('imgs4'):\n",
        "  os.mkdir('imgs4')\n",
        "\n",
        "n=0\n",
        "for filepath in glob.iglob('/content/NORMAL/*.csv'):\n",
        "#for filepath in glob.iglob('/content/drive/My Drive/P-108 s0013_rem.csv'):\n",
        "    #print(filepath)\n",
        "    img_array = genfromtxt(filepath, delimiter=',')\n",
        "    #print(img_array.shape)\n",
        "    #scipy.misc.toimage(image_array, cmin=0.0, cmax=1.0).save('outfile.jpg')\n",
        "    im = Image.fromarray(img_array)\n",
        "    image00= np.repeat(img_array[:, :, np.newaxis], 3, axis=2)\n",
        "    #cv2_imshow(image)\n",
        "    print(image00.shape)\n",
        "    #plt.imshow(im)\n",
        "    #new_image = im.resize((299, 299))\n",
        "    #plt.imshow(new_image)\n",
        "    image00 = resize(image00, (256, 256, 3))\n",
        "    n=n+1\n",
        "    print (n)\n",
        "    #plt.imshow(image00)\n",
        "    image00 = ((image00 * 255).astype(np.uint8))\n",
        "    print(image00.size)\n",
        "    print(image00.shape)\n",
        "    # plt.imsave('output'+str(n),image00 , format='png' )\n",
        "    image00 = Image.fromarray(image00.astype(np.uint8))\n",
        "    image00.save(\"imgs4/check\"+str(n)+\".jpeg\", \"JPEG\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGh4UntBhlnS"
      },
      "source": [
        "# !rm -rf imgs\n",
        "# !rm *.png\n",
        "# !rm output*\n",
        "!cp -r imgs4/* /content/drive/My\\ Drive/travi4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OTvhFQouEBj"
      },
      "source": [
        "# !rm -rf imgs\n",
        "# !rm *.png\n",
        "# !rm output*\n",
        "!cp -r imgs/* drive/My\\ Drive/travi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZBTI2wxBpM"
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "from numpy import genfromtxt\n",
        "\n",
        "g = open('/content/1.csv','r')\n",
        "temp = genfromtxt(g, delimiter = ',')\n",
        "im = Image.fromarray(temp).convert('RGB')\n",
        "pix = im.load()\n",
        "rows, cols = im.size\n",
        "for x in range(cols):\n",
        "  for y in range(rows):\n",
        "    print (str(x) + \" \" + str(y))\n",
        "    pix[x,y] = (int(temp[y,x] // 256 // 256 % 256),int(temp[y,x] // 256  % 256),int(temp[y,x] % 256))\n",
        "im.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94xlgEye7qq7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout,BatchNormalization,Activation,MaxPooling2D\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from  tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlAlJTgw-QBi"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train',  # this is the target directory\n",
        "        target_size=(256,256),  # all images will be resized to 512*512\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')  # since we use binary_crossentropy\n",
        "\n",
        "# print(train_generator.class_indices)\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/test',\n",
        "        target_size=(256,256),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEsFgqZG-X2Y"
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Dropout\n",
        "from tensorflow.keras.layers import Input\n",
        "input_tensor = Input(shape=(256, 256, 3))\n",
        "\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model =MobileNetV2(input_tensor=input_tensor,weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "#x=Dropout(0.5)\n",
        "\n",
        "\n",
        "x = Dense(512, activation='relu')(x)\n",
        "#x=Flatten()(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "x=Dropout(0.5)(x)\n",
        "\n",
        "predictions = Dense(6, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZMBdwwRng0l"
      },
      "source": [
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)\n",
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 150 layers and unfreeze the rest:\n",
        "for layer in model.layers[0:130]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[130:]:\n",
        "   layer.trainable = True\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZWLpRPGEKfe"
      },
      "source": [
        "print(\"Number of layers in the base model: \", len(model.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc1TSrvMeX91"
      },
      "source": [
        "def scheduler(epochs, lr):\n",
        "  if epochs < 5:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtebZBM5dQfG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        verbose=0,\n",
        "        min_lr=0.01\n",
        "    )\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1)\n",
        "tensorboard=tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,\n",
        "    update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
        "    embeddings_metadata=None\n",
        ")\n",
        "learning_scheduler=tf.keras.callbacks.LearningRateScheduler(\n",
        "    scheduler, verbose=0\n",
        ")\n",
        "    # early stopping when `val_loss` stops improving\n",
        "\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qg1SL4IWe92"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEPFZdVkCfrd"
      },
      "source": [
        "epochs=50\n",
        "nb_train_samples=3435\n",
        "nb_validation_samples=935"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtn6K7Hf-c5W"
      },
      "source": [
        "\n",
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=int(nb_train_samples / batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=int(nb_validation_samples / batch_size),\n",
        "    callbacks=[reduce_lr,learning_scheduler])\n",
        "    \n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GVaYiaezUkO"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1,verbose=True)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y_true = validation_generator.labels\n",
        "#print(validation_generator.labels)\n",
        "#print(y_pred)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = ['ALMI','AMI','ASMI','IMI','INFERIOR','NORMAL']\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY9MZT6PVnRI"
      },
      "source": [
        "tf.keras.utils.plot_model(model,'mobilenetv2.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X-lFg0XkuFi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X='/content/testecg'\n",
        "X_train, X_test = train_test_split(X, test_size = 0.3, random_state = 0)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dFyDkP80TwI"
      },
      "source": [
        "import cv2\n",
        "im = cv2.imread('/content/test/travi/check.png')\n",
        "\n",
        "print(type(im))\n",
        "# <class 'numpy.ndarray'>\n",
        "\n",
        "print(im.shape)\n",
        "print(type(im.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fL6bvkf4hTE"
      },
      "source": [
        "model.save('MobileV2Net.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWHb0C2J28LE"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "\n",
        "    for (idx, c_label) in enumerate(target_names): # all_labels: no of the labels, for ex. \n",
        "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
        "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    #c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='orange', label='ALMI')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='green', label='AMI')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='red', label='ASMI')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='pink', label='IMI')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='blue', label='INFERIOR')\n",
        "        c_ax.plot(fpr, tpr, 'b-', linestyle='--',color='grey', label='NORMAL')\n",
        "\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "# calling\n",
        "validation_generator.reset() # resetting generator\n",
        "y_pred = model.predict_generator(validation_generator, verbose = True)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "multiclass_roc_auc_score(validation_generator.classes, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAJ2-arNHoGE"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "data_x = []\n",
        "data_y = []\n",
        "\n",
        "path = '/content/train'\n",
        "classname = 0\n",
        "for folder in os.listdir(path):\n",
        "    labels = [0. for _ in range(6)]\n",
        "    labels = np.array(labels)\n",
        "    for image in os.listdir(os.path.join(path, folder)):\n",
        "        image_path = os.path.join(path, folder, image)\n",
        "        img = keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "        img_array = keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = img_array/255.0\n",
        "        data_x.append(img_array)\n",
        "        labels[classname] = 1.\n",
        "        data_y.append(labels)\n",
        "    classname += 1\n",
        "\n",
        "data_x, data_y = np.array(data_x), np.array(data_y)\n",
        "data_x, data_y = shuffle(data_x, data_y, random_state=seed)\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "cvscores = []\n",
        "hists = []\n",
        "\n",
        "for train, test in kfold.split(data_x, data_y):\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "    for x in train:\n",
        "        train_x.append(data_x[x])\n",
        "        train_y.append(data_y[x])\n",
        "    for x in test:\n",
        "        test_x.append(data_x[x])\n",
        "        test_y.append(data_y[x])\n",
        "\n",
        "    train_x, train_y, test_x, test_y = np.array(train_x), np.array(train_y), np.array(test_x), np.array(test_y)\n",
        "    model = keras.models.load_model('/content/MobileV2Net.h5')\n",
        "    model.compile(Adam(lr=0.0001), 'categorical_crossentropy',  metrics=['accuracy'])\n",
        "    history = model.fit(train_x, train_y, validation_split=0.2, epochs=30, batch_size=32, verbose=2)\n",
        "    hists.append(history)\n",
        "    scores = model.evaluate(test_x, test_y, verbose = 2)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "\n",
        "    \n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "\n",
        "model.save('kfold_mobilenet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMthQNXrDrcZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=len(hists), ncols=2, figsize=(20,20))\n",
        "\n",
        "for i, history in enumerate(hists):\n",
        "    history = history.history\n",
        "    ax[i, 0].plot(history['accuracy'], label='Accuracy')\n",
        "    ax[i, 0].plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax[i, 0].legend(loc='upper left')\n",
        "    # plt.xlabel('Epochs')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    ax[i, 0].set_title('Accuracy')\n",
        "    ax[i, 1].plot(history['loss'], label='Loss')\n",
        "    ax[i, 1].plot(history['val_loss'], label='Validation Loss')\n",
        "    ax[i, 1].legend(loc='upper left')\n",
        "    # ax[i, 1].xlabel('Epochs')\n",
        "    # ax[i, 1].ylabel('Loss')\n",
        "    ax[i, 1].set_title('Loss')\n",
        "    plt.xlabel('Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPwCeMCVDr4B"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "\n",
        "    for (idx, c_label) in enumerate(target_names): # all_labels: no of the labels, for ex. \n",
        "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
        "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
        "    c_ax.legend(loc='lower right')\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "import os\n",
        "if os.path.exists('mobilenet.h5'):\n",
        "    model = keras.models.load_model('mobilenet.h5')\n",
        "# calling\n",
        "validation_generator.reset() # resetting generator\n",
        "y_pred = model.predict(validation_generator, verbose = True)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "multiclass_roc_auc_score(validation_generator.classes, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}